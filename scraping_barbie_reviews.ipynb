{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Barbie IMDB reviews dataset: 5 columns (rating, user, date, headline, description) and 1471 rows of data"
      ],
      "metadata": {
        "id": "IhJo4Ca56TFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1/10 Reviews"
      ],
      "metadata": {
        "id": "6YI5KDCgzzzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "def get_reviews(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    while True:\n",
        "        # Make a request to the current URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "        page_html = response.text\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        for item in page_soup.find_all(\"div\", class_=\"lister-item-content\"):\n",
        "            rating = item.find(\"span\", class_=\"rating-other-user-rating\").text\n",
        "            user = item.find(\"span\", class_=\"display-name-link\").text\n",
        "            date = item.find(\"span\", class_=\"review-date\").text\n",
        "            title = item.find(\"a\", class_=\"title\").text.strip()\n",
        "            description = item.find(\"div\", class_=\"text show-more__control\").text.strip()\n",
        "\n",
        "            reviews.append([rating, user, date, title, description])\n",
        "\n",
        "        # Check if there's a \"Load More\" button\n",
        "        load_more_button = page_soup.find(\"div\", class_=\"load-more-data\")\n",
        "        if load_more_button:\n",
        "            # Get the \"data-key\" attribute from the button\n",
        "            data_key = load_more_button['data-key']\n",
        "\n",
        "            # Update the URL to include the data-key for the next request\n",
        "            url = f'https://www.imdb.com/title/tt1517268/reviews/_ajax?sort=curated&dir=desc&ratingFilter=1&paginationKey={data_key}'\n",
        "        else:\n",
        "            break  # No more \"Load More\" button\n",
        "\n",
        "    return reviews\n",
        "\n",
        "my_url = 'https://www.imdb.com/title/tt1517268/reviews?sort=curated&dir=desc&ratingFilter=1'      #ni link review for raitng 1/10 je\n",
        "reviews = get_reviews(my_url)\n",
        "# Open a CSV file for writing\n",
        "csv_filename = 'barbie_reviews1.csv'                                             ##################################name file 1, 2, 3 etc.\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write headers to the CSV file\n",
        "    csv_writer.writerow(['Rating', 'User', 'Date', 'Headline', 'Description'])\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    csv_writer.writerows(reviews)\n",
        "\n",
        "print(f\"\\n++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "print(f\"\\nScraped data rating 1/10 has been saved to {csv_filename}\")   #check csv file to see the scraped data\n"
      ],
      "metadata": {
        "id": "qKuegpBCwsk5",
        "outputId": "d708161b-6e01-42f4-d2d4-5ff03188c49d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Scraped data rating 1/10 has been saved to barbie_reviews1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2/10 Reviews"
      ],
      "metadata": {
        "id": "RUSeJqqPz5Kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "def get_reviews(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    while True:\n",
        "        # Make a request to the current URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "        page_html = response.text\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        for item in page_soup.find_all(\"div\", class_=\"lister-item-content\"):\n",
        "            rating = item.find(\"span\", class_=\"rating-other-user-rating\").text\n",
        "            user = item.find(\"span\", class_=\"display-name-link\").text\n",
        "            date = item.find(\"span\", class_=\"review-date\").text\n",
        "            title = item.find(\"a\", class_=\"title\").text.strip()\n",
        "            description = item.find(\"div\", class_=\"text show-more__control\").text.strip()\n",
        "\n",
        "            reviews.append([rating, user, date, title, description])\n",
        "\n",
        "        # Check if there's a \"Load More\" button\n",
        "        load_more_button = page_soup.find(\"div\", class_=\"load-more-data\")\n",
        "        if load_more_button:\n",
        "            # Get the \"data-key\" attribute from the button\n",
        "            data_key = load_more_button['data-key']\n",
        "\n",
        "            # Update the URL to include the data-key for the next request\n",
        "            url = f'https://www.imdb.com/title/tt1517268/reviews/_ajax?sort=curated&dir=desc&ratingFilter=2&paginationKey={data_key}'\n",
        "        else:\n",
        "            break  # No more \"Load More\" button\n",
        "\n",
        "    return reviews\n",
        "\n",
        "my_url = 'https://www.imdb.com/title/tt1517268/reviews?sort=curated&dir=desc&ratingFilter=2'      #ni link review for rating 2/10 je\n",
        "reviews = get_reviews(my_url)\n",
        "# Open a CSV file for writing\n",
        "csv_filename = 'barbie_reviews2.csv'                                             ##################################name file 1, 2, 3 etc.\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write headers to the CSV file\n",
        "    csv_writer.writerow(['Rating', 'User', 'Date', 'Headline', 'Description'])\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    csv_writer.writerows(reviews)\n",
        "\n",
        "print(f\"\\n++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "print(f\"\\nScraped data rating 2/10 has been saved to {csv_filename}\")   #check csv file to see the scraped data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3HPFJBN1L9G",
        "outputId": "16ae696a-5314-41e0-ebf6-997447ecaed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Scraped data rating 2/10 has been saved to barbie_reviews2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3/10 Reviews"
      ],
      "metadata": {
        "id": "pBwjz4krz8l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "def get_reviews(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    while True:\n",
        "        # Make a request to the current URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "        page_html = response.text\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        for item in page_soup.find_all(\"div\", class_=\"lister-item-content\"):\n",
        "            rating = item.find(\"span\", class_=\"rating-other-user-rating\").text\n",
        "            user = item.find(\"span\", class_=\"display-name-link\").text\n",
        "            date = item.find(\"span\", class_=\"review-date\").text\n",
        "            title = item.find(\"a\", class_=\"title\").text.strip()\n",
        "            description = item.find(\"div\", class_=\"text show-more__control\").text.strip()\n",
        "\n",
        "            reviews.append([rating, user, date, title, description])\n",
        "\n",
        "        # Check if there's a \"Load More\" button\n",
        "        load_more_button = page_soup.find(\"div\", class_=\"load-more-data\")\n",
        "        if load_more_button:\n",
        "            # Get the \"data-key\" attribute from the button\n",
        "            data_key = load_more_button['data-key']\n",
        "\n",
        "            # Update the URL to include the data-key for the next request\n",
        "            url = f'https://www.imdb.com/title/tt1517268/reviews/_ajax?sort=curated&dir=desc&ratingFilter=3&paginationKey={data_key}'\n",
        "        else:\n",
        "            break  # No more \"Load More\" button\n",
        "\n",
        "    return reviews\n",
        "\n",
        "my_url = 'https://www.imdb.com/title/tt1517268/reviews?sort=curated&dir=desc&ratingFilter=3'\n",
        "reviews = get_reviews(my_url)\n",
        "# Open a CSV file for writing\n",
        "csv_filename = 'barbie_reviews3.csv'                                             ##################################name file 1, 2, 3 etc.\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write headers to the CSV file\n",
        "    csv_writer.writerow(['Rating', 'User', 'Date', 'Headline', 'Description'])\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    csv_writer.writerows(reviews)\n",
        "\n",
        "print(f\"\\n++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "print(f\"\\nScraped data rating 3/10 has been saved to {csv_filename}\")   #check csv file to see the scraped data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_H6EFe20Equ",
        "outputId": "05f91350-e3ae-4055-919d-096766c3dafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Scraped data rating 3/10 has been saved to barbie_reviews3.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4/10 Reviews"
      ],
      "metadata": {
        "id": "Cg6NqmUA0BCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "def get_reviews(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    while True:\n",
        "        # Make a request to the current URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "        page_html = response.text\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        for item in page_soup.find_all(\"div\", class_=\"lister-item-content\"):\n",
        "            rating = item.find(\"span\", class_=\"rating-other-user-rating\").text\n",
        "            user = item.find(\"span\", class_=\"display-name-link\").text\n",
        "            date = item.find(\"span\", class_=\"review-date\").text\n",
        "            title = item.find(\"a\", class_=\"title\").text.strip()\n",
        "            description = item.find(\"div\", class_=\"text show-more__control\").text.strip()\n",
        "\n",
        "            reviews.append([rating, user, date, title, description])\n",
        "\n",
        "        # Check if there's a \"Load More\" button\n",
        "        load_more_button = page_soup.find(\"div\", class_=\"load-more-data\")\n",
        "        if load_more_button:\n",
        "            # Get the \"data-key\" attribute from the button\n",
        "            data_key = load_more_button['data-key']\n",
        "\n",
        "            # Update the URL to include the data-key for the next request\n",
        "            url = f'https://www.imdb.com/title/tt1517268/reviews/_ajax?sort=curated&dir=desc&ratingFilter=4&paginationKey={data_key}'\n",
        "        else:\n",
        "            break  # No more \"Load More\" button\n",
        "\n",
        "    return reviews\n",
        "\n",
        "my_url = 'https://www.imdb.com/title/tt1517268/reviews?sort=curated&dir=desc&ratingFilter=4'\n",
        "reviews = get_reviews(my_url)\n",
        "# Open a CSV file for writing\n",
        "csv_filename = 'barbie_reviews4.csv'                                             ##################################name file 1, 2, 3 etc.\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write headers to the CSV file\n",
        "    csv_writer.writerow(['Rating', 'User', 'Date', 'Headline', 'Description'])\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    csv_writer.writerows(reviews)\n",
        "\n",
        "print(f\"\\n++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "print(f\"\\nScraped data rating 4/10 has been saved to {csv_filename}\")\n",
        "#check csv file to see the scraped data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_PTSSko0E9r",
        "outputId": "35a2d2a7-5c71-4b19-918b-d76f3262b3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Scraped data rating 4/10 has been saved to barbie_reviews4.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5/10 Reviews"
      ],
      "metadata": {
        "id": "HtPCavIX0CRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "def get_reviews(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    reviews = []\n",
        "\n",
        "    while True:\n",
        "        # Make a request to the current URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "        page_html = response.text\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        for item in page_soup.find_all(\"div\", class_=\"lister-item-content\"):\n",
        "            rating = item.find(\"span\", class_=\"rating-other-user-rating\").text\n",
        "            user = item.find(\"span\", class_=\"display-name-link\").text\n",
        "            date = item.find(\"span\", class_=\"review-date\").text\n",
        "            title = item.find(\"a\", class_=\"title\").text.strip()\n",
        "            description = item.find(\"div\", class_=\"text show-more__control\").text.strip()\n",
        "\n",
        "            reviews.append([rating, user, date, title, description])\n",
        "\n",
        "        # Check if there's a \"Load More\" button\n",
        "        load_more_button = page_soup.find(\"div\", class_=\"load-more-data\")\n",
        "        if load_more_button:\n",
        "            # Get the \"data-key\" attribute from the button\n",
        "            data_key = load_more_button['data-key']\n",
        "\n",
        "            # Update the URL to include the data-key for the next request\n",
        "            url = f'https://www.imdb.com/title/tt1517268/reviews/_ajax?sort=curated&dir=desc&ratingFilter=5&paginationKey={data_key}'\n",
        "        else:\n",
        "            break  # No more \"Load More\" button\n",
        "\n",
        "    return reviews\n",
        "\n",
        "my_url = 'https://www.imdb.com/title/tt1517268/reviews?sort=curated&dir=desc&ratingFilter=5'\n",
        "reviews = get_reviews(my_url)\n",
        "# Open a CSV file for writing\n",
        "csv_filename = 'barbie_reviews5.csv'\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write headers to the CSV file\n",
        "    csv_writer.writerow(['Rating', 'User', 'Date', 'Headline', 'Description'])\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    csv_writer.writerows(reviews)\n",
        "\n",
        "print(f\"\\n++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "print(f\"\\nScraped data rating 5/10 has been saved to {csv_filename}\")\n",
        "#check csv file to see the scraped data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwsuwhgM0Fdr",
        "outputId": "00c151c6-eb2f-4d37-bd68-b3bc9b557874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Scraped data rating 5/10 has been saved to barbie_reviews5.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6/10 Reviews"
      ],
      "metadata": {
        "id": "-qZ_W7DM0DRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "def get_reviews(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    while True:\n",
        "        # Make a request to the current URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "        page_html = response.text\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        for item in page_soup.find_all(\"div\", class_=\"lister-item-content\"):\n",
        "            rating = item.find(\"span\", class_=\"rating-other-user-rating\").text\n",
        "            user = item.find(\"span\", class_=\"display-name-link\").text\n",
        "            date = item.find(\"span\", class_=\"review-date\").text\n",
        "            title = item.find(\"a\", class_=\"title\").text.strip()\n",
        "            description = item.find(\"div\", class_=\"text show-more__control\").text.strip()\n",
        "\n",
        "            reviews.append([rating, user, date, title, description])\n",
        "\n",
        "        # Check if there's a \"Load More\" button\n",
        "        load_more_button = page_soup.find(\"div\", class_=\"load-more-data\")\n",
        "        if load_more_button:\n",
        "            # Get the \"data-key\" attribute from the button\n",
        "            data_key = load_more_button['data-key']\n",
        "\n",
        "            # Update the URL to include the data-key for the next request\n",
        "            url = f'https://www.imdb.com/title/tt1517268/reviews/_ajax?sort=curated&dir=desc&ratingFilter=6&paginationKey={data_key}'\n",
        "        else:\n",
        "            break  # No more \"Load More\" button\n",
        "\n",
        "    return reviews\n",
        "\n",
        "my_url = 'https://www.imdb.com/title/tt1517268/reviews?sort=curated&dir=desc&ratingFilter=6'\n",
        "reviews = get_reviews(my_url)\n",
        "# Open a CSV file for writing\n",
        "csv_filename = 'barbie_reviews6.csv'                                             ##################################name file 1, 2, 3 etc.\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write headers to the CSV file\n",
        "    csv_writer.writerow(['Rating', 'User', 'Date', 'Headline', 'Description'])\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    csv_writer.writerows(reviews)\n",
        "\n",
        "print(f\"\\n++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "print(f\"\\nScraped data rating 6/10 has been saved to {csv_filename}\")\n",
        "#check csv file to see the scraped data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAYk7R0M0GBc",
        "outputId": "62911d0c-714f-4af4-937d-508bbc53ad54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Scraped data rating 6/10 has been saved to barbie_reviews6.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7/10 Reviews"
      ],
      "metadata": {
        "id": "N5eCnFKZ0GTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "def get_reviews(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    while True:\n",
        "        # Make a request to the current URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "        page_html = response.text\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        for item in page_soup.find_all(\"div\", class_=\"lister-item-content\"):\n",
        "            rating = item.find(\"span\", class_=\"rating-other-user-rating\").text\n",
        "            user = item.find(\"span\", class_=\"display-name-link\").text\n",
        "            date = item.find(\"span\", class_=\"review-date\").text\n",
        "            title = item.find(\"a\", class_=\"title\").text.strip()\n",
        "            description = item.find(\"div\", class_=\"text show-more__control\").text.strip()\n",
        "\n",
        "            reviews.append([rating, user, date, title, description])\n",
        "\n",
        "        # Check if there's a \"Load More\" button\n",
        "        load_more_button = page_soup.find(\"div\", class_=\"load-more-data\")\n",
        "        if load_more_button:\n",
        "            # Get the \"data-key\" attribute from the button\n",
        "            data_key = load_more_button['data-key']\n",
        "\n",
        "            # Update the URL to include the data-key for the next request\n",
        "            url = f'https://www.imdb.com/title/tt1517268/reviews/_ajax?sort=curated&dir=desc&ratingFilter=7&paginationKey={data_key}'\n",
        "        else:\n",
        "            break  # No more \"Load More\" button\n",
        "\n",
        "    return reviews\n",
        "\n",
        "my_url = 'https://www.imdb.com/title/tt1517268/reviews?sort=curated&dir=desc&ratingFilter=7'\n",
        "reviews = get_reviews(my_url)\n",
        "# Open a CSV file for writing\n",
        "csv_filename = 'barbie_reviews7.csv'                                             ##################################name file 1, 2, 3 etc.\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write headers to the CSV file\n",
        "    csv_writer.writerow(['Rating', 'User', 'Date', 'Headline', 'Description'])\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    csv_writer.writerows(reviews)\n",
        "\n",
        "print(f\"\\n++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "print(f\"\\nScraped data rating 7/10 has been saved to {csv_filename}\")\n",
        "#check csv file to see the scraped data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg3-YpOZ0Hlk",
        "outputId": "726f01e6-0067-4dcd-c20d-1ebd03aea0fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Scraped data rating 7/10 has been saved to barbie_reviews7.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8/10 Reviews"
      ],
      "metadata": {
        "id": "sr-DuQnH0HzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "def get_reviews(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    while True:\n",
        "        # Make a request to the current URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "        page_html = response.text\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        for item in page_soup.find_all(\"div\", class_=\"lister-item-content\"):\n",
        "            rating = item.find(\"span\", class_=\"rating-other-user-rating\").text\n",
        "            user = item.find(\"span\", class_=\"display-name-link\").text\n",
        "            date = item.find(\"span\", class_=\"review-date\").text\n",
        "            title = item.find(\"a\", class_=\"title\").text.strip()\n",
        "            description = item.find(\"div\", class_=\"text show-more__control\").text.strip()\n",
        "\n",
        "            reviews.append([rating, user, date, title, description])\n",
        "\n",
        "        # Check if there's a \"Load More\" button\n",
        "        load_more_button = page_soup.find(\"div\", class_=\"load-more-data\")\n",
        "        if load_more_button:\n",
        "            # Get the \"data-key\" attribute from the button\n",
        "            data_key = load_more_button['data-key']\n",
        "\n",
        "            # Update the URL to include the data-key for the next request\n",
        "            url = f'https://www.imdb.com/title/tt1517268/reviews/_ajax?sort=curated&dir=desc&ratingFilter=8&paginationKey={data_key}'\n",
        "        else:\n",
        "            break  # No more \"Load More\" button\n",
        "\n",
        "    return reviews\n",
        "\n",
        "my_url = 'https://www.imdb.com/title/tt1517268/reviews?sort=curated&dir=desc&ratingFilter=8'\n",
        "reviews = get_reviews(my_url)\n",
        "# Open a CSV file for writing\n",
        "csv_filename = 'barbie_reviews8.csv'                                             ##################################name file 1, 2, 3 etc.\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write headers to the CSV file\n",
        "    csv_writer.writerow(['Rating', 'User', 'Date', 'Headline', 'Description'])\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    csv_writer.writerows(reviews)\n",
        "\n",
        "print(f\"\\n++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "print(f\"\\nScraped data rating 8/10 has been saved to {csv_filename}\")\n",
        "#check csv file to see the scraped data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INbustC70JEV",
        "outputId": "92d227df-2350-49c3-8525-ef1cd7ed9b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Scraped data rating 8/10 has been saved to barbie_reviews8.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9/10 Reviews"
      ],
      "metadata": {
        "id": "ckq1oXZd0JXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "def get_reviews(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    while True:\n",
        "        # Make a request to the current URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "        page_html = response.text\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        for item in page_soup.find_all(\"div\", class_=\"lister-item-content\"):\n",
        "            rating = item.find(\"span\", class_=\"rating-other-user-rating\").text\n",
        "            user = item.find(\"span\", class_=\"display-name-link\").text\n",
        "            date = item.find(\"span\", class_=\"review-date\").text\n",
        "            title = item.find(\"a\", class_=\"title\").text.strip()\n",
        "            description = item.find(\"div\", class_=\"text show-more__control\").text.strip()\n",
        "\n",
        "            reviews.append([rating, user, date, title, description])\n",
        "\n",
        "        # Check if there's a \"Load More\" button\n",
        "        load_more_button = page_soup.find(\"div\", class_=\"load-more-data\")\n",
        "        if load_more_button:\n",
        "            # Get the \"data-key\" attribute from the button\n",
        "            data_key = load_more_button['data-key']\n",
        "\n",
        "            # Update the URL to include the data-key for the next request\n",
        "            url = f'https://www.imdb.com/title/tt1517268/reviews/_ajax?sort=curated&dir=desc&ratingFilter=9&paginationKey={data_key}'\n",
        "        else:\n",
        "            break  # No more \"Load More\" button\n",
        "\n",
        "    return reviews\n",
        "\n",
        "my_url = 'https://www.imdb.com/title/tt1517268/reviews?sort=curated&dir=desc&ratingFilter=9'\n",
        "reviews = get_reviews(my_url)\n",
        "# Open a CSV file for writing\n",
        "csv_filename = 'barbie_reviews9.csv'                                             ##################################name file 1, 2, 3 etc.\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write headers to the CSV file\n",
        "    csv_writer.writerow(['Rating', 'User', 'Date', 'Headline', 'Description'])\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    csv_writer.writerows(reviews)\n",
        "\n",
        "print(f\"\\n++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "print(f\"\\nScraped data rating 9/10 has been saved to {csv_filename}\")\n",
        "#check csv file to see the scraped data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wxa8lh9v0Kst",
        "outputId": "4dbb5267-6065-49c4-fdce-7b1b1e235ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Scraped data rating 9/10 has been saved to barbie_reviews9.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10/10 Reviews"
      ],
      "metadata": {
        "id": "EaesL3gZ0K49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "def get_reviews(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    while True:\n",
        "        # Make a request to the current URL\n",
        "        response = requests.get(url, headers=headers)\n",
        "        page_html = response.text\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        for item in page_soup.find_all(\"div\", class_=\"lister-item-content\"):\n",
        "            rating = item.find(\"span\", class_=\"rating-other-user-rating\").text\n",
        "            user = item.find(\"span\", class_=\"display-name-link\").text\n",
        "            date = item.find(\"span\", class_=\"review-date\").text\n",
        "            title = item.find(\"a\", class_=\"title\").text.strip()\n",
        "            description = item.find(\"div\", class_=\"text show-more__control\").text.strip()\n",
        "\n",
        "            reviews.append([rating, user, date, title, description])\n",
        "\n",
        "        # Check if there's a \"Load More\" button\n",
        "        load_more_button = page_soup.find(\"div\", class_=\"load-more-data\")\n",
        "        if load_more_button:\n",
        "            # Get the \"data-key\" attribute from the button\n",
        "            data_key = load_more_button['data-key']\n",
        "\n",
        "            # Update the URL to include the data-key for the next request\n",
        "            url = f'https://www.imdb.com/title/tt1517268/reviews/_ajax?sort=curated&dir=desc&ratingFilter=10&paginationKey={data_key}'\n",
        "        else:\n",
        "            break  # No more \"Load More\" button\n",
        "\n",
        "    return reviews\n",
        "\n",
        "my_url = 'https://www.imdb.com/title/tt1517268/reviews?sort=curated&dir=desc&ratingFilter=10'\n",
        "reviews = get_reviews(my_url)\n",
        "# Open a CSV file for writing\n",
        "csv_filename = 'barbie_reviews10.csv'                                             ##################################name file 1, 2, 3 etc.\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write headers to the CSV file\n",
        "    csv_writer.writerow(['Rating', 'User', 'Date', 'Headline', 'Description'])\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    csv_writer.writerows(reviews)\n",
        "\n",
        "print(f\"\\n++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "print(f\"\\nScraped data rating 10/10 has been saved to {csv_filename}\")\n",
        "#check csv file to see the scraped data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh9oDGiE159i",
        "outputId": "bd8d2ec2-5076-4e72-86f3-fe29657f3849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Scraped data rating 10/10 has been saved to barbie_reviews10.csv\n"
          ]
        }
      ]
    }
  ]
}